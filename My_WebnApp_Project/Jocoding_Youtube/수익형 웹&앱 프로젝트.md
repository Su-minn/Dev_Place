# 실전 수익형 웹, 앱 서비스 동물상 테스트 만들기



from - [JoCoding | Youtube](https://www.youtube.com/watch?v=ZTJjW7XuHIY&list=PLU9-uwewPMe2-vtJAgWB6SNhHcTjJDgEO)



## Chap 1. 파이썬으로 텍스트 및 이미지 크롤링



- 이번 챕터에서 다룰 내용 - 텍스트 & 이미지 크롤링
  - 크롤링 : 인터넷에 있는 정보 중 우리가 원하는 것만 골라서 자동으로 수집해주는 기술

1)  환경 설정

- Python 개발 환경 설정 (구름 IDE)

2) 예제 무작정 실행하기

- 위키피디아 크롤링 예제

3) 다른 작업에 응용하기

- 네이버 실시간 검색순위 크롤링하여 텍스트 저장

4) 완전 쉬운 라이브러리 사용

- 동물상 연예인 사진 한방에 크롤링



### 1) 환경설정

- 프로젝트를 진행하기 위해서는,  
  컴퓨터에 개발 언어를 설치하고 관련 라이브러리들을 설치해야한다   
  하지만 모든 것을 그대로 컴퓨터에 설치하게되면, 나중에 다른 작업을 할 때 버전간 충돌이 발생하는 등 문제가 생길 수 있기에, 보통 가상환경을 구성하여 그 곳에 필요 내용을 설치하게 된다 (venv, vitrualenv, conda 등)

- 이 강의에서는 구름 IDE를 사용할 예정이다  
  구름 IDE는 개발환경이 갖춰진 가상의 컴퓨터를 제공해주는 것이라 별도의 언어 설치 과정이 필요하지 않고, 또 다른 작업을 할 때는 가상컴퓨터를 새롭게 만들어서 작업하면 되므로 버전간 충돌이 일어날 일도 없다  

- 보통 로컬에 환경설정을 하는 것은 귀찮은 경우가 많다  
  하지만, 이런 클라우드 기반 IDE 서비스를 잘 이용하면 그런 시간이 단축되고 편리하게 개발에만 집중할 수 있다
- 구름 IDE는 무료로 이용할 수 있는 클라우드 IDE 서비스 중 상당히 괜찮은 편이며, 이 프로젝트 진행을 위해 다른 IDE를 사용하거나 로컬 환경을 사용해도 상관 없다
- 공식 홈페이지 - [ide.goorm.io](https://ide.goorm.io/)

- 회원가입 후, 오른 쪽 위 대시보드 버튼 클릭
- 새 컨테이너 생성 클릭  
  - 컨테이너, 즉 우리가 사용할 가상 컴퓨터에 대한 환경을 설정하는 공간
    - 컨테이너 이름 - crwaling
    - 지역 - 서울
    - public
    - template
    - 배포 not used
    - 언어 스택 python
  - 선택 후, 생성
  - 컨테이너 실행 누르기
- 왼쪽에는 탐색기, 아래에는 명령어 입력창, 위에 코드 편집기 존재  
  이와 같이, 코딩에 필요한 화면을 한 곳에 몰아 넣은 것을 통합개발환경 (IDE)라 부른다
- 기본으로 index.py 파일이 생성되있는 상태  
  아래 명령창에 python index.py와 같이 파일명을 입력해주면 잘 실행되는지 확인 가능



### 2) 예제 무작정 실행하기

- 이제 본격적으로 크롤링 시작
- beautiful soup라는 라이브러리 사용 예정  
  HTML 및 XML 구문을 분석하기 위한 파이썬 패키지
- beautiful soup [검색해서 나오는 위키백과](https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser))에 들어가서,  
  있는 코드를 그대로 복붙 해보자
- command + S로 저장을 하고, 실행시켜보자
  - 터미널창에서 윗쪽 화살표를 누르면 이전 명령어가 뜬다
- 실행 결과로, 모듈을 못찾는다는 에러가 발생
- beautifulsoup를 설치하지 않았기 때문이다, 설치해보자
  - 터미널에 `pip install bs4` 를 입력해보자
- 미리 만들어진 기능, 라이브러리를 설치해서 쓸 때,  
  파이썬의 경우에는 파이썬 패키지 관리자 pip를 이용해서,  
  `pip install 패키지명`  을 입력하면 프로그램 다운부터 설치까지 자동으로 이루어진다
- 어떤 언어로 프로그램을 작성할 때, 언어 자체의 기능으로 구현할 수도 있지만, 같은 기능을만들어 놓은 라이브러리가 있다면 무조건 가져다 쓰는 편이 훨씬 빠르고 효율적이다
- 다시 실행을 시켜보면, 결과값이 잘나오는 것을 볼 수있다

- 코드를 살펴보면

  - with A as B 구문 이 보이는데,  
    A의 결과 값을 B에 담겠다는 의미

    ```python
    # (1)번 표현
    with urlopen('https://en.wikipedia.org/wiki/Main_Page') as response:
      
    # (2)번 표현
    reponse = urlopen('https://en.wikipedia.org/wiki/Main_Page')
    
    # 두가지 표현 모두 동일
    ```



### 3) 다른 작업에 응용하기

- 네이버 실시간 검색어 부분을 크롤링해보자

- 네이버 전체 사이트에서 실시간 검색어 부분만 골라서 텍스트만 가져와야하므로,  
  이 부분이 어떻게 생겼는지 특징을 파악하고, 그 부분을 선택해서 가져오도록 해야한다

- 네이버에 들어가서,  
  option + command + i 를 입력하여 개발자도구를 열고,  
  shift + command + C 를 이용해서 셀렉트 모드로 변경하여,  
  검색 순위 코드가 어떻게 생겼는지 규칙을 파악하자

- 모두 span 태그 안에 있지만, span 태그를 모두 가져오면  
  필요 없는 부분들도 다 가져오게된다.  
  그래서 더 구체적으로 살펴보면 모두 class가 "ah_k"로 되어있음을 알 수 있다  
  2020 기준 class 명은 "keyword" 

- 그러면 span 태그이면서, class 이름이 위에 해당하는 내용을 가져오면 되겠다고  
  가설을 세울 수 있다

- 위키백과에서 복사해온 코드부분을 변형하자

  - urlopen 부분에는 우리가 가져오고자 하는 웹사이트인  
    https://www.naver.com/ 를 넣어주자
  - 그리고 태그 부분을 변경해줄 예정인데,  
    find_all 함수가 헷갈리면, 구글에 beautifulsoup를 검색해서, 공식 document를 참고하자
  - beautifulSoup_documentation 웹사이트에서  
    command + f 로 select를 검색해보자
  - 나오게되는 CSS Selector 안의 .select 함수는 크롤링에서 많이 사용되는 함수이다
  - CSS에서 어떤 요소를 선택하고 꾸며줄지 결정하는 문법과 동일하다
    - ex) `soup.select("body a")`  
      <body> 태그 안에 있는 자손 중에 <a> 태그를 선택하라는 CSS문법과 동일
  - 이 내용을 기존의 코드에 적용시켜보자

  - `soup.find_all('a'):` 부분에서 우리가 원하는 내용을 선택하도록,  
    soup.select("span.ah_k") 로 변경해주자

  - 그리고, 출력하는 print 부분에서 href는 span태그에는 없기 때문에 지우고  
    `print(anchor)`로 출력해보자

  - beautifulsoup documentation을 참고해서, 우리가 원하는 결과인  
    텍스트만 나오도록 하기위해, anchor 뒤에 .get_text()를 추가해주자

  - 원하는 결과가 잘 나옴을 확인할 수 있고, 더 깔끔하게 순위를 추가해주기 위해 아래 코드로 print를 변경해주자

  - ```python
    for anchor in soup.select("span.ah_k"):
      print(str(i) + "위 : " + anchor.get_text())
      i = i + 1
    ```

- 추가로 터미널 화면 출력뿐 아니라, 파일로 내용을 저장해보자

- 구글에다가 python write text file 이라고 검색

  - 코드를 외워 쓰는 것이 아니라, 구글에 언어 이름 + 하고자하는 행위를 검색하면 거의 모든 해결책이 나온다

- 구글의 결과로 얻은 코드를 복붙하자

  - 원하는 방법으로 변경할 계획
  - 코드를 읽어보면, 파일을 열고, 파일에 반복문을 돌며 write 해주고,  
    close해주는 간단한 코드
  - 이 내용을 적용시켜보자

- f = open 코드는 for문 위로 올려주고, 우리는 C드라이브를 쓰는 것이 아니라,  
  클라우드 공간에 있기에, `새파일.txt` 로 경로를 바꾸어주자  
  index와 같은 경로에 파일이 생성될 예정

- data 에 값을 넣는 부분은, 우리가 print로 출력 하던 내용을  
  data에 넣어주는 것으로 변경

- 그리고 data에 값이 들어가면  `f.write(data)` 로 작성

- 이후 for문이 끝나면 `f.close()`  로 닫아주기 

- 결과 값을 보니, 줄바꿈이 없어서 지저분하기 때문에 data 뒤에   
   `data = str(i) + "위 : " + anchor.get_text() + "\n"` 로 개행 추가해주기

- 이미지 크롤링도 마찬가지로 웹페이지의 구조를 보고 src의 규칙을 찾아서 크롤링하면 된다

- 하지만, 이미지를 크롤링하는 일과 같이 많은 사람이 이미 했을 것으로 보이는 일은  
  라이브러리로 만들어져있다

- 코딩에서 웬만하면 '이건 누가 했을 것 같은데?' 라는 생각이 들면 무조건 누가 이미 했다



### 4) 완전 쉬운 라이브러리 사용

- 구글에 python google image search and download 라 검색
- 결과를 보면, google_images_download라는 라이브러리가 이미 존재  
  - `pip install google_images_downlaod` 를 통해 라이브러리 설치 가능
  - 페이지를 보면 example이 존재하고, code sample에 가서 코드를 복사하자
  - 구름 파일 탐색기 부분에 마우스 오른쪽 클릭, 새 파일을 눌러서 생성하자
  - google.py라고 이름을 설정하고 복사한 코드를 붙여 넣어주자
    - 코드를 실행시켜보면, download 폴더 안에,  
      beaches, polar bears, baloons가 20장씩 다운되어있음을 확인 가능하다
    - 코드와 비교해보면, arguments 부분의 각 keyword가  
      limit인 20장씩 다운되었음을 비교분석 할 수 있다
- 이와 같이, 잘만들어진 라이브러리를 사용하면, 편하게 원하는 결과를 얻을 수 있다
- google_images_download 문서에서, input arguments를 살펴보자
  - 라이브러리에 사용할 수 있는 각종 옵션의 정보 존재
  - 우리는 teachablemachine을 사용할 것이기에,  
    여기서 extension으로 검색을 해보면 format이라는 옵션 내용이 있고,  
    jpg, gif 같은 확장자를 지정해서 원하는 확장자를 다운 받을 수 있게하는 옵션이 있다
  - teachable machine에서 gif는 잘 인식을 못하므로,  
    jpg파일만 지정해서 다운받도록 하자
- 다시 코드로 가서 arguments 부분에 `"format : jpg"` 라는 부분을 추가해주자
- 다운받다가 오류가 나는 경우에는, 해당 키워드 하나만 넣고 다시 돌려서 다운받아주자
- 지금은 클라우드에 다운받아진 것이기에, 로컬에 받고 싶으면   
  해당 폴더를 마우스 우측 클릭, 파일 다운르도, zip를 클릭하여, 압축파일로 받을 수 있다



## Chap 2. 웹캠 없이 Teachable Machine으로 나와 닮은 동물상 찾기



## Chap 3. 디자인코드 변환 도구 제플린(zeplin)의 개념과 실전 반응형 웹 앱 제작하기



## Chap 4. 코딩 없이 댓글 기능 구현하기



## Chap 5. SNS 공유 구현하고 Github & Netlify 배호 자동화 및



## Chap 6. 안드로이드와 아이폰 어플 만들기